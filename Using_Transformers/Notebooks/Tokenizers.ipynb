{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff41c85-aac7-49d1-b109-6aa23710304c",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f69919-f961-410d-8e24-6dab3b5e97a3",
   "metadata": {},
   "source": [
    ">Tokenizers need to convert our text inputs to numerical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e99df2c-085a-4c10-981f-b6e284a2a023",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/word_based_tokenization.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574650a8-ef58-4533-8f81-443ff208e8e5",
   "metadata": {},
   "source": [
    ">There are different ways to split the text or extra rules for punctuation. For example, we could use whitespace to tokenize the text into words by applying Python’s split() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cea3e2-a970-4034-a4f6-2d38b71dfe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64775918-48cb-4b4d-a61b-49d92f22af0f",
   "metadata": {},
   "source": [
    ">Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3971d5f4-1395-45ca-9612-375679db6f9d",
   "metadata": {},
   "source": [
    "There are charachter based tokenization and word based tokenization  \n",
    "**charachter based tokenization**  \n",
    "1. very large amount of tokens to be processed by our model whereas a word would only be a single token with a word-based tokenizer\n",
    "2. Each character doesn’t mean a lot on its own,in **Chinese**, for example, each character carries more information   \n",
    "  **This has two primary benefits:**  \n",
    "1. The vocabulary is much smaller.  \n",
    "2. There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
    "\n",
    "**word based tokenization** \n",
    "1. 500,000 words in the English language map from each word to an input ID we’d need to keep track of that many IDs\n",
    "2. “run” and “running”, “dog” and “dogs”, model will not see as similar initially.\n",
    "3. It’s generally a bad sign if you see that the tokenizer is producing a lot of ”[UNK]” or ”<unk>”. tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785c289d-4126-462d-8c35-3b8fad7b2e48",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/character_based_tokenization.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dd0079-89dc-457e-81cf-61eefb0b4116",
   "metadata": {},
   "source": [
    "## Subword Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d29b324-8ac5-442e-b5c5-e6b28adadeb4",
   "metadata": {},
   "source": [
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.  \n",
    "“annoyingly”considered a rare word,decomposed into “annoying” and “ly”.   \n",
    "Both appear frequently as standalone subwords,while meaning of “annoyingly” is kept by the composite meaning of “annoying” and “ly”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981edae-fed4-4dab-9625-c4f23049a5fc",
   "metadata": {},
   "source": [
    ">from_pretrained() and save_pretrained().Loading and saving tokenizers is as simple as it is with models.These methods will load or save the algorithm used by the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "759ed781-60c5-4c61-8635-7571ad504d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer=BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f583f57-e5d6-4dea-921f-f710f69c03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d087e390-f1ec-49ff-940a-b647f1da048a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7993, 170, 13809, 23763, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7290333d-a986-4d69-9096-6f0224a4a3fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('directory_on_my_computer/tokenizer_config.json',\n",
       " 'directory_on_my_computer/special_tokens_map.json',\n",
       " 'directory_on_my_computer/vocab.txt',\n",
       " 'directory_on_my_computer/added_tokens.json',\n",
       " 'directory_on_my_computer/tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87452eb2-0d4f-4bf3-b8a6-58fc98823f41",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "##### Translating text to numbers is known as encoding. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216edbfe-83ef-468d-b2ae-6362a8f25822",
   "metadata": {},
   "source": [
    ">The second step converts those tokens into numbers, so we can build a tensor out of them and feed to the model.the tokenizer has a vocabulary, which is the part we download when we instantiate it with the from_pretrained() method. Again, we need to use the same vocabulary used when the model was pretrained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6875548-3f54-4bfd-8781-7fd4d149a82a",
   "metadata": {},
   "source": [
    "### Tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6138265b-c751-4761-9ff1-30fa50d5c526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f42e5d5a-8253-4144-a3f6-a21b135eeec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ecf71e6-9302-455e-af26-48265db30c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Using', 'a', 'Trans', '##former', 'network', 'is', 'simple']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens=tokenizer.tokenize(sequence)\n",
    "print(tokens)#split into two tokens: transform and ##er.subword tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f806e18-7d22-4a88-8cff-ac0a8ac8eb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', '’', 've', 'been', 'waiting', 'for', 'a', 'Hu', '##gging', '##F', '##ace', 'course', 'my', 'whole', 'life', '.', '”', 'and', '“', 'I', 'hate', 'this', 'so', 'much', '!']\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I’ve been waiting for a HuggingFace course my whole life.” and “I hate this so much!\"\n",
    "tokens=tokenizer.tokenize(sequence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4cdb6c-51ea-4b6b-9f6d-ec323012e937",
   "metadata": {},
   "source": [
    "### From tokens to input IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2312be7-fc03-44f1-a4d9-ab9d6af35259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7993, 170, 13809, 23763, 2443, 1110, 3014]\n"
     ]
    }
   ],
   "source": [
    "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834b03b-20f6-47e6-8b95-9e3c60a89b17",
   "metadata": {},
   "source": [
    "### Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe2f4a44-afb7-471b-9f9b-acc6d6f754ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Using a Transformer network is simple'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_string=tokenizer.decode(ids)\n",
    "decoded_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7e1db8-9906-4c03-b56e-5562fb0a3f21",
   "metadata": {},
   "source": [
    ">Note that the decode method not only converts the indices back to tokens, but also groups together the tokens(sub word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695b31c-e0a4-447c-8db2-5abbdbeef07e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
