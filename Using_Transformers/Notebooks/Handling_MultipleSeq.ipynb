{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579c787f-6056-4573-aa92-f892fe7d72ba",
   "metadata": {},
   "source": [
    "## Handling multiple sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13255159-7dcc-4fbb-b70a-a1be45b38beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d51ef99-fc50-44ff-a433-16abeac5f44b",
   "metadata": {},
   "source": [
    ">In the previous exercise you saw how sequences get translated into lists of numbers. Let’s convert this list of numbers to a tensor and send it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e2a87c-3975-435c-b50b-4945e5e68f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75420ce2-833c-4d4c-89c6-75681a604532",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "model=AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06a271c0-f7aa-4d6e-b58e-6fd839535aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence=\"I've been waiting for a HuggingFace course my whole life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9222dcba-53e6-497d-a376-a7ad7aa6393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=tokenizer.tokenize(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "831f64ba-92e7-41ac-bca9-8910115d14ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids=tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids=torch.tensor([ids])#extra dim[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf9e5be7-6899-4140-be47-83dcd24e826d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tensor([[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,\n",
      "          2026,  2878,  2166,  1012]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5acdafe-e682-4a9b-a34b-ef656a291836",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "424fdd37-25e7-4b02-8a8d-24be78f82764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"logits:\",output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f5b824-bf31-489b-bb41-f99284da6070",
   "metadata": {},
   "source": [
    ">Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "227e9b00-8d12-415a-80d0-574456ac2f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]\n",
    "batched_input_ids=torch.tensor(batched_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "186317a6-70e6-4d07-a10d-8ebe1baa2393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-2.7276,  2.8789],\n",
      "        [-2.7276,  2.8789]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_1=model(batched_input_ids)\n",
    "\n",
    "print(\"logits:\",output_1.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3a65cc-b9b1-46bf-8e43-1f8284567c0d",
   "metadata": {},
   "source": [
    "## Padding the inputs\n",
    ">we’ll use padding to make our tensors have a rectangular shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52beba4e-a2d1-4602-ab59-cdf0fecbe826",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "706f8ee4-c8a6-4cad-b611-fd5a554c13b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple example\n",
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a8a222e-4ac2-4dd0-979c-754158039e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 1.3374, -1.2163]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8d67b-98c6-42fc-b550-151842a3f05e",
   "metadata": {},
   "source": [
    ">Our batched predictions: the second row should be the same as the logits for the second sentence, but we’ve got completely different values!\n",
    ">   \n",
    ">Transformer models is attention layers that contextualize each token. These will take into account the padding tokens since they attend to all of the tokens of a sequence   \n",
    "> we need to tell those attention layers to ignore the padding tokens. This is done by using an attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c453107-2051-485b-86fc-8b759afb3240",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask=[\n",
    "    [1,1,1],\n",
    "    [1,1,0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7d745b2-6ca0-4da0-b6a9-5e1cf992f773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5694, -1.3895],\n",
      "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "outputs=model(torch.tensor(batched_ids),attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6e89a-953f-4daf-a607-53ca81028919",
   "metadata": {},
   "source": [
    ">now check the second rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2880-c79e-4310-a9fe-fef589b435de",
   "metadata": {},
   "source": [
    "## Truncation\n",
    ">With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f452b13-04e8-4995-9d05-67ddf6a71beb",
   "metadata": {},
   "source": [
    ">Solution:\n",
    ">1. Use a model with a longer supported sequence length.\n",
    " 2. Truncate your sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3587b6a4-5e37-47b1-a0cc-d631f333a8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Otherwise, we recommend you truncate your sequences by specifying the max_sequence_length parameter:\n",
    "max_sequence_length=1024\n",
    "sequence = sequence[:max_sequence_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf817442-0aa8-4a8b-a386-8910fdf253b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
